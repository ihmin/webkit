<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>WebGPU Safari Texture via img.decode</title>
    <style>
:root {
  color-scheme: light dark;
}

    </style>
  </head>
  <body>
  <canvas></canvas>
  </body>
  <script>
      globalThis.testRunner?.waitUntilDone();

async function webgpu({canvas, loadType}) {
  const adapter = await navigator.gpu?.requestAdapter();
  const device = await adapter?.requestDevice();
  if (!device) {
    fail('need a browser that supports WebGPU');
    return;
  }
  device.addEventListener('uncapturederror', e => console.error(e.error.message));

  // Get a WebGPU context from the canvas and configure it
  const context = canvas.getContext('webgpu');
  const presentationFormat = navigator.gpu.getPreferredCanvasFormat();
  context.configure({
    device,
    format: presentationFormat,
  });

  const module = device.createShaderModule({
    label: 'our hardcoded textured quad shaders',
    code: `
      struct VSOutput {
        @builtin(position) pos: vec4f,
        @location(0) uv: vec2f,
      }
      @vertex fn vs(@builtin(vertex_index) ndx: u32) -> VSOutput {
        let pos = array(
          vec2f(-1, -1),
          vec2f( 1, -1),
          vec2f(-1,  1),
          vec2f(-1,  1),
          vec2f( 1, -1),
          vec2f( 1,  1),
        );
        return VSOutput(
          vec4f(pos[ndx], 0, 1),
          vec2f(pos[ndx] * 0.5 + 0.5),
        );
      }

      @group(0) @binding(0) var ourSampler: sampler;
      @group(0) @binding(1) var ourTexture: texture_2d<f32>;

      @fragment fn fs(fsInput: VSOutput) -> @location(0) vec4f {
        return textureSample(ourTexture, ourSampler, fsInput.uv);
      }
    `,
  });

  const pipeline = device.createRenderPipeline({
    label: 'hardcoded textured quad pipeline',
    layout: 'auto',
    vertex: {
      module,
      entryPoint: 'vs',
    },
    fragment: {
      module,
      entryPoint: 'fs',
      targets: [
        {
          format: presentationFormat,
        },
      ],
    },
  });

  const texture = await new Promise(resolve => {
    getSource(loadType, function() {
      const source = this;
      const texture = device.createTexture({
        format: 'rgba8unorm',
        size: [source.width, source.height],
        usage: GPUTextureUsage.TEXTURE_BINDING |
              GPUTextureUsage.RENDER_ATTACHMENT |
              GPUTextureUsage.COPY_DST |
              GPUTextureUsage.COPY_SRC,
      });

      device.queue.copyExternalImageToTexture(
        { source },
        { texture },
        { width: source.width, height: source.height },
      );

      resolve(texture);
    });
  });

  const sampler = device.createSampler();

  const bindGroup = device.createBindGroup({
    layout: pipeline.getBindGroupLayout(0),
    entries: [
      { binding: 0, resource: sampler },
      { binding: 1, resource: texture.createView() },
    ],
  });

  const renderPassDescriptor = {
    label: 'our basic canvas renderPass',
    colorAttachments: [
      {
        // view: <- to be filled out when we render
        loadOp: 'clear',
        storeOp: 'store',
      },
    ],
  };

  // Get the current texture from the canvas context and
  // set it as the texture to render to.
  renderPassDescriptor.colorAttachments[0].view =
      context.getCurrentTexture().createView();

  const encoder = device.createCommandEncoder({
    label: 'render quad encoder',
  });
  const pass = encoder.beginRenderPass(renderPassDescriptor);
  pass.setPipeline(pipeline);
  pass.setBindGroup(0, bindGroup);
  pass.draw(6);
  pass.end();

  const buffer = device.createBuffer({
    size: 16,
    usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
  });
  encoder.copyTextureToBuffer(
    { texture },
    { buffer },
    [4, 1],
  );

  const commandBuffer = encoder.finish();
  device.queue.submit([commandBuffer]);

  await buffer.mapAsync(GPUMapMode.READ);
  const pixels = new Uint8Array(buffer.getMappedRange());
  console.log(`\ntexData first 4 texels:\n${formatPixelData(pixels)}`);
  buffer.unmap();
  globalThis.testRunner?.dumpAsText();
  globalThis.testRunner?.notifyDone();
}

function formatPixelData(pixels) {
  return Array
    .from(pixels)
    .map((v, i) => `${i % 4 === 0 ? '  [' : ''}${v.toString(16).padStart(2, '0')}${i % 4 === 3 ? ']\n' : ', '}`).join('');
}

function getSource(loadType, callback) {
    const source = new Image();
  source.crossOrigin = '*';
  source.src = './noise-with-gama-and-chrm-chunks.png';
  if (loadType === 'decode') {
    (async () => {
      await source.decode();
      callback.call(source);
    })();
  } else {
    source.addEventListener('load', callback, { once: true });
  }
}

function fail(msg) {
  // eslint-disable-next-line no-alert
  alert(msg);
}
onload = async () => {
          webgpu({ loadType: 'decode', canvas: document.querySelector('canvas') });
};
  </script>
</html>
